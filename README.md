# Random-Forest-Classification-with-or-without-PCA
This study utilizes the MNIST data set to test how using PCA as a preliminary to random forest classifier will impact the predictive accuracy of the model as well as the time spent on model development and implementation.**

The first 60 thousands observations of MNIST data set are used as train set while the rest is holdout as test set. We first train a random forest classifier (Model 1) on the train set and evaluate the model's performance on the holdout test set with F1 score. The time spent on developing and implementing Model 1 is recorded and stored, so dose the F1 value. In the second stage, we utilize Pipeline to conduct PCA transformation on the train set before train the transformed train set with a random forest classifier (Model 2). The performance of Model 2 is evaluated on the holdout test set as well with F1 score as performance index. The time spent on developing and implementing Model 2 is also recorded and stored.**

When comparing the two models on their predictive accuracy (F1 score) and development/implementation time, we notice that Model 1 which preserve all the dimensions of the original MNIST data set, achieves higher accuracy with a higher F1 score than Model 2 which preserve 95% of the variability in the original explanatory variables. Model 2 also takes 353.6% more time to develop and implement than Model 1.**

Based on the result, it is recommended to the management not to use PCA as preliminary approach for random forest classification as it neither improves the predictive accuracy nor decrease time on model development and implementation. There are a few reasons which lead to the result. First, random forest classifier performs well on high dimensions without transformation/regularization as it's transformation invariant. While PCA can reduce ddimensionality of the data set dramatically, the default setting of max_feature parameter in randomforestcalssifier is sqrt(features). This means that the difference between the number of features to consider when looking for the best split before or after PCA transformation is smaller than it may have appeared. Additionally, random forest classifier iterates over possible splits that optimize the criterion. By reducing the number of features, we might have made it more difficult to find the split, leading the algorithm to perform more iterations in order to find a good split.**

It is important to point out that the conclusion drawn from this study doesn’t necessarily apply to other machine learning classification algorithms, such as SVM or neutral network which are sensitive to data transformation. For algorithms like SVM and neutral network, PCA transformation will most likely improve the model’s predictive performance. Take neutral network as an example, most of the real-world data samples used to train artificial neural networks (ANNs) consist of correlated information caused by overlapping input instances. Correlation in sampled data normally creates confusion over ANNs during the learning process and thus, degrades their generalization capability. (Mohamad-Saleh & Hoyle 2008). Since PCA can eliminate correlated information through dimensionality reduction, therefore, using PCA as preliminary process to neutral network classifier will likely improve the model’s performance.
